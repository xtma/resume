% \textbf{Uber Eats \hfill  San Francisco, CA} \par
% \textit{Data Analyst} \hfill Sep 2021--Dec 2021 \par
% \begin{itemize}
% 	\item Queried behavioral data on 2M eaters for segmentation via a random forests, PCA, and clustering algorithms.
% 	\item Modeled effect of discounts on Eats pass conversion and provided strategy recommendations for each cluster.
% \end{itemize} \par

\begin{itemize}[leftmargin=*]
    \item \textbf{Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning.} \textit{Yiqin Yang*, \textcolor{violet}{Xiaoteng Ma*}, Chenghao Li, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun Yang and Qianchuan Zhao.} Advances in Neural Information Processing Systems (NeurIPS), 2021. \textcolor{red}{(Spotlight, accept rate 3.1\%)} 
    %\href{https://arxiv.org/abs/2106.03400}{\textcolor{blue}{(arXiv link)}} 
    \par
    \textcolor{violet}{Summary:} We demonstrate current offline RL algorithms are ineffective in multi-agent systems due to the accumulated extrapolation error. We propose a concise algorithm named ICQ, which successfully controls the extrapolation error within a reasonable range and achieves the state-of-the-art performance in the challenging multi-agent offline tasks (StarCraft II). This work is recognized as the first work to study multi-agent offline settings in DRL. 

    \item \textbf{Mildly Conservative Q-Learning for Offline Reinforcement Learning.} \textit{Jiafei Lyu*, \textcolor{violet}{Xiaoteng Ma*}, Xiu Li, Zongqing Lu.} Advances in Neural Information Processing Systems (NeurIPS), 2022. \textcolor{red}{(Spotlight)} %%\href{https://arxiv.org/abs/2206.04745}{\textcolor{blue}{(arXiv link)}} 
    \par
    \textcolor{violet}{Summary:} We introduce the concept of Mild Conservatism to offline reinforcement learning, focusing on maintaining generalization and accurately learning policies without inflating the value of out-of-distribution actions. We propose MCQ, a method that embodies this concept, surpassing previous approaches in performance and demonstrating enhanced generalization, notably when shifting from offline to online environments.
        
    \item \textbf{Efficient Multi-agent Reinforcement Learning by Planning.} \textit{Qihan Liu*, Jianing Ye*, \textcolor{violet}{Xiaoteng Ma*}, Jun Yang, Bin Liang, Chongjie Zhang.} International Conference on Learning Representations (ICLR), 2024.
    \par
    \textcolor{violet}{Summary:} We introduce MAZero, the first application of MuZero to complex multi-agent environments, enhancing sample efficiency and outperforming model-free approaches in MARL. Our method demonstrates superior performance on the SMAC benchmark through a centralized model with MCTS, an innovative network for distributed execution, and two novel techniques that optimize search efficiency in deterministic environments with large action spaces.
\end{itemize}
